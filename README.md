
# AI-Powered Competitor Intelligence System
This project leverages AI and web scraping techniques to track and analyze job postings from multiple online platforms, such as LinkedIn, Indeed, Glassdoor, and more. The goal is to gather job market data,provide valuable insights into hiring trends, in-demand skills, and competitor activities.

Features: Job Posting Scraping: Automatically scrape job listings from multiple sources.
Competitor Analysis: Analyze job postings from competitors to understand their hiring strategies.

Trend Analysis:  Identify patterns in job requirements, including skills, experience, and job titles.

AI Integration: Use OpenAI and NLP techniques to process and summarize job data.

Real-Time Tracking: Continuously monitor job openings and provide real-time updates.

Notion Integration: Store and display analyzed data in Notion for easy viewing and reporting.
Technologies Used

FastAPI: For building the web API to serve the application.
BeautifulSoup: For scraping job listings from web pages.

OpenAI API: For AI-driven insights and text analysis.

Python: For backend development and data processing.

Docker: For containerizing the application.
## Authors

- [@vishalrajput29](https://github.com/vishalrajput29)


## Badges

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)



## Demo
YOU CAN SEE THE DEMO OF APP BY CLICKING ON THIS LINK

https://drive.google.com/file/d/1pqY46ji0Bmrt2uOnIC_FV_8o-c7s7WyC/view?usp=sharing
## Wireframe

(![Screenshot 2024-11-27 232122](https://github.com/user-attachments/assets/a1ec1f7a-e98a-434b-8963-c7bf24b5fbcb))


## Installation

Install my-project with npm

```bash
  npm install my-project
  cd my-project
  pip install -r requirements.txt
  python api.py
  git clone https://github.com/yourusername/competitor-intelligence.git
cd competitor-intelligence

```
    
## Lessons Learned

Building this project allowed me to deepen my understanding of several key concepts related to web scraping, machine learning, and API development. Here are the major takeaways:

Web Scraping with Python: I gained hands-on experience with scraping data from various job posting platforms using Python libraries like BeautifulSoup and Requests. I learned how to handle dynamic web pages and automate the extraction of relevant job data (titles, skills, companies, locations, etc.).

Integrating OpenAI API: I explored the use of the OpenAI API for natural language processing tasks, such as extracting meaningful insights from job postings and summarizing hiring trends. This taught me how to effectively use AI to process large amounts of textual data and generate actionable insights.

FastAPI and Web Development: I became familiar with FastAPI for building high-performance web APIs. FastAPIâ€™s ease of use for creating RESTful APIs, combined with automatic generation of interactive documentation via Swagger, was a great learning experience.

Docker for Containerization: I learned how to dockerize a Python-based application to ensure consistent and isolated execution environments across different systems. By using Docker, I could containerize my entire project, making it easy to deploy and run in production environments.

Project Management and Integration: I also learned how to integrate and manage various services (web scraping, NLP, APIs) in a single application and ensure smooth communication between them. The integration of job data into Notion helped me understand how to connect different platforms using APIs.


## Environment Variables

To run this project, you will need to add the following environment variables to your .env file

`API_KEY`

`ANOTHER_API_KEY`


## ðŸš€ About Me
Driven B-Tech (ECE) student with a passion for data science, specializing in machine learning and statistical modeling to extract
actionable insights and drive impactful business decisions. With a focus on machine learning, data visualization, and statistical
modeling, I am committed to extracting insights from complex data sets and driving business outcomes through data-driven decision
making.

